Paper,Method name,Category,Dataset / benchmark,Metrics used,Paper performance,Baselines compared,Source
Continual Knowledge Adaptation for Reinforcement Learning (NeurIPS 2025),—,Partial/Adaptive update (knowledge adaptation),—,Average return / success rate over tasks; forgetting/backward transfer (typical CRL),Improves continual adaptation vs prior baselines (per paper),Prior continual RL baselines (unspecified here),neurips.cc (paper page)
Tackling Continual Offline RL through Selective Weights Activation on Aligned Spaces (NeurIPS 2025),Selective Weights Activation (alignment via VQ),Partial/Adaptive update,Continual World (CW10); MuJoCo; D4RL (offline),Average return; offline RL normalized scores,Reports gains over standard offline CRL baselines,SAC/BC baselines; CRL regularization variants (per paper),arXiv (paper link in README)
Knowledge Retention for Continual Model-Based Reinforcement Learning (ICML 2025),Generative world model with frozen prior + new learner,Partial/Adaptive update (model-based),—,Average return; model-based CRL retention/forgetting,Shows better retention while preserving plasticity,Model-based RL + CL baselines,arXiv (paper link in README)
Continual Reinforcement Learning by Planning with Online World Models (ICML 2025),Online world model + MPC planning,Partial/Adaptive update (model-based),Proposed continual bench environment (per paper),Average return; forward/forgetting,Improves over value-based/model-free baselines on proposed bench,"Model-free CL, model-based ablations",OpenReview (paper page)
Mitigating Plasticity Loss in Continual RL by Reducing Churn (ICML 2025),Churn reduction via NTK insights,Partial/Adaptive update (optimization),Gym Control; Procgen; MinAtar,Average episodic return; plasticity/forgetting measures,Improves plasticity vs Fast TRAC-like setups,Fast TRAC; standard CL baselines,OpenReview (paper page)
Position: Lifetime tuning is incompatible with continual RL (ICML 2025),—,— (position paper),—,—,—,—,OpenReview (paper page)
Prevalence of Negative Transfer in Continual RL: Analyses and a Simple Baseline (ICLR 2025),Dual-actor baseline (periodic reset + distillation),Partial/Adaptive update,Meta-World; DMC Control; Atari,Average return/success; forward/backward transfer,Shows large negative transfer in fine-tuning; proposed baseline mitigates,Fine-tune; EWC/SI/LwF-like; behavior cloning variants,OpenReview (paper page)
Fast TRAC: A Parameter-Free Optimizer for Lifelong RL (NeurIPS 2024),Fast TRAC (parameter-free optimizer),Partial/Adaptive update (optimizer),Procgen; Atari; Gym Control,Average episodic return; sample efficiency; forgetting,Outperforms Adam/SGD baselines in nonstationary settings,Adam; SGD; other optimizers + CRL regularizers,OpenReview (paper page)
Parseval Regularization for Continual RL (NeurIPS 2024),Parseval Regularization,Partial/Adaptive update (regularization),—,Average return; plasticity/forgetting measures,Improves optimization stability; better plasticity/retention,EWC; L2; SI; naive fine-tune,OpenReview (paper page)
Self-composing policies for scalable continual RL (ICML 2024),Self-composing Policies (growing + attention),Partial/Adaptive update (architectural growth),Meta-World; Atari,Average return/success; forward transfer; forgetting,Strong plasticity and average performance vs PackNet-style growth,PackNet; progressive nets; fine-tune; EWC/SI,ICML paper page (linked in README)
Loss of plasticity in deep continual learning (Nature 2024),Continual Propagation (algorithm),Partial/Adaptive update,Supervised + RL studies (various),Plasticity measures; performance across sequences,Demonstrates plasticity loss; continual propagation mitigates partially,Standard training; CL regularizers,Nature
Replay-enhanced Continual RL (TMLR 2023),RECALL (normalized Q + policy distillation),Full policy update (SAC-based with replay),Continual World (Meta-World CW10),Average success rate; forward/backward transfer; forgetting,Significantly better than pure replay and competitive with SOTA CRL,Perfect memory replay; EWC/SI/L2; SAC fine-tune; other CL methods,OpenReview
Building a Subspace of Policies for Scalable Continual Learning (ICLR 2023),Policy Subspace Expansion,Partial/Adaptive update (subspace),—,Average return; model size vs performance,Balances performance and capacity growth,Progressive nets; PackNet; fine-tune,arXiv
A Definition of Continual Reinforcement Learning (NeurIPS 2023),—,— (definition / framework),—,—,—,—,arXiv
Prediction and Control in Continual RL (NeurIPS 2023),Two-timescale Value Decomposition,Partial/Adaptive update (value-based),—,Average return; prediction error; forgetting,Improved continual value estimation and control over baselines,DQN/SAC variants; CL regularizers,arXiv
COOM: A Game Benchmark for Continual RL (NeurIPS 2023),COOM benchmark,— (benchmark),Image-based game benchmark (TensorFlow impl),Average return; forgetting; forward/backward transfer,— (benchmark introduction),Includes baseline agents on COOM,OpenReview
Continual Task Allocation in Meta-Policy Network via Sparse Prompting (ICML 2023),Sparse Prompting (meta-policy masks),Meta learning,—,Average return/success across tasks,Improved task allocation and performance,Meta-policy baselines; CL regularizers,arXiv
Task-Agnostic Continual RL: Gaining Insights and Overcoming Challenges (CoLLAs 2023),—,— (analysis/insights),—,—,—,—,arXiv
CoMPS: Continual Meta Policy Search (ICLR 2022),CoMPS,Meta learning,—,Average return; adaptation speed; forgetting,Improved fast adaptation with retention,MAML-style; PEARL; CL baselines,arXiv
Lifelong Robotic RL by Retaining Experiences (CoLLAs 2022),Importance-weighted replay,Partial/Adaptive update (replay weighting),Simulated robotics suite,Average return/success; plasticity/retention,Improves over naive replay,Naive replay; fine-tune; EWC/SI,PMLR
Disentangling Transfer in Continual RL (NeurIPS 2022),BC + SAC components (analysis + method),Full policy update (SAC-based),—,Average return; forward/backward transfer,Combines improvements via behavior cloning for better transfer,SAC variants; CL regularizers,arXiv
Towards Evaluating Adaptivity of Model-Based RL Methods (ICML 2022),Adaptivity analysis (no new algorithm),— (evaluation/analysis),Tasks with local reward changes; replay size ablations,Adaptivity to reward changes; return,Identifies trade-off between replay size and adaptivity,Multiple model-based RL methods,arXiv
Continual World: A Robotic Benchmark for Continual RL (NeurIPS 2021),CW10 benchmark,— (benchmark),Meta-World based sequences (CW10/CW20),Final average success (AvgPerf); Forward Transfer (FT); Forgetting (F),Shows many CL methods struggle on FT; defines standard metrics,EWC; SI; LwF; Experience Replay; PackNet; fine-tune,arXiv (2105.10919)
Towards Continual RL: A Review and Perspectives (JAIR 2020),—,— (review),—,—,—,—,arXiv/JAIR
Policy Consolidation for Continual RL (ICML 2019),Policy Consolidation (PC),Partial/Adaptive update (regularization via self-distillation),"Continuous control (e.g., Mujoco/Gym); self-play setting",Average episodic return; stability/forgetting across task switches,Improves over PPO baselines in alternating tasks and self-play,PPO variants; EWC/SI-style where applicable,arXiv (1902.00255)
Continual RL with Complex Synapses (ICML 2018),Complex Synapses,Partial/Adaptive update (synaptic model),Tabular RL experiments,Average return; forgetting measure in tabular tasks,Mitigates catastrophic forgetting in tabular settings,Tabular RL baselines; simple rehearsal,arXiv
State Abstractions for Lifelong RL (ICML 2018),State Abstractions,Partial/Adaptive update (theory + methods),—,Sample complexity; return bounds; empirical return,Provides theoretical guarantees; empirical support,Standard RL with/without abstraction,PMLR
