Category,Paper/Method,Year/Venue,Key Approach,Benchmarks,Metrics,Baselines,Performance
Full Policy Updates,CSP (Continual Subspace of Policies),2024,Maintains a subspace of policies as a convex hull of anchors for full policy derivation via convex combinations.,CORA (2D environments, sequences of 15 tasks).,Average Performance (A_N), Forgetting (FG), Forward Transfer (FT),EWC, P&C,A_N ~0.85 (vs. EWC ~0.70); FG reduced by 20-30%; FT improved by leveraging subspaces.
Full Policy Updates,ClonEx-SAC,2024,Full policy updates with best-return exploration from prior SAC policies, cloning and fully retraining.,Continual World (robotic manipulation, 10 tasks).,A_N, FG, Sample Efficiency (SE),Standard SAC, random exploration,A_N ~0.78 (vs. SAC ~0.65); SE 15-20% fewer samples; FG lowered by 10-15%.
Full Policy Updates,SANE (Self-Activating Neural Ensembles),2024,Dynamically activates and fully updates module ensembles via UCB selection.,L2Explorer (3D PCG world, 5 tasks).,A_N, FG, FT, Backward Transfer (BT),PNN, EWC,A_N ~0.82 (vs. PNN ~0.75); FG ~0.12 (vs. ~0.25); FT ~0.15; BT ~0.10.
Full Policy Updates,VBLRL (Variational Bayesian Lifelong RL),2024,Full updates to dynamics models using variational Bayesian inference.,COOM (ViZDoom-based, sequences of 4-16 tasks).,A_N, FG, SE,MOLe, EWC,A_N ~0.80 (vs. MOLe ~0.72); FG reduced by 25%; SE 2x fewer samples.
Full Policy Updates,3RL (Replay-based Recurrent RL),2024,Uses RNNs for full policy updates with recurrent memory replay.,CORA (4 environments, sequences of 4-15 tasks).,A_N, FG, FT,CLEAR, RePR,A_N ~0.85 (vs. CLEAR ~0.75); FG ~0.10; FT ~0.20.
Full Policy Updates,Continual-Dreamer,2024,Full world model updates with reservoir sampling for latent dynamics.,COOM (7 sequences of 4-16 tasks).,A_N, FG, FT,Dreamer, EWC,A_N ~0.83 (vs. Dreamer ~0.70); FG ~0.15; FT ~0.18.
Full Policy Updates,Parseval Regularization for Continual Reinforcement Learning,2024/NeurIPS,Regularization encouraging orthogonality in weight matrices; full updates to PPO/RPO layers.,Gridworld (changing goals); CARL (LunarLander, DMCQuadruped); MetaWorld20-10 (10 tasks).,Avg Success Rate/Return, Performance Profiles (1 - CDF of scores),Layer Norm, SnP, Regen, W-Regen; PPO/RPO base,MetaWorld: ~0.6 success (vs. base ~0.2); Gridworld: ~800 return (vs. ~200-300); LunarLander: ~200-250 (vs. ~100); DMCQuadruped: ~100-150 (vs. ~0-50).
Partial/Adaptive Parameter Updates,Continual RL by Planning with Online World Models (OA),2025/ICML,Sparse updates to a unified world model using FTL; policies via MPC with CEM.,Continual Bench (6 robotic tasks, 600 episodes).,Average Performance (AP), Regret (Reg),Fine-tuning, SI, Coreset, Perfect Memory, EWC, PackNet,AP 72.93% (vs. Perfect Memory 73.09%, Coreset 61.83%); Reg 27.62% (vs. Coreset 30.83%).
Partial/Adaptive Parameter Updates,LEGION,2025/Nature Machine Intelligence,Hierarchical DPMM + memoVB for task inference; selective SAC policy updates.,10 robotic tasks (1M steps each); long-horizon; few-shot recall (5 tasks, 3 loops).,Avg Success Rate, Forgetting (F), Forward Transfer (FT), Few-Shot Recall Improvement,Reservoir, Perfect Memory, A-GEM; MTRL,Avg Success 0.84 (vs. MTRL 0.94); F 0.0; FT 0.10; Few-Shot ~11.96% (1st vs. 2nd), ~21.36% (1st vs. 3rd).
Partial/Adaptive Parameter Updates,ARC-RL,2024/ICLR (OpenReview),Decouples policy via encoder-decoder; adapts decoder structurally with EWC.,MiniGrid (action space changes); Procgen-Bigfish.,Continual Return (R), Forgetting (F), Forward Transfer (T),IND, FT, EWC, Online-EWC, CLEAR, Mask,MiniGrid Expansion: R 0.90 (vs. FT 0.86); F -0.02; T 0.57. Contraction: R 0.80 (vs. FT 0.52); F 0.04; T 0.60. Procgen: R 10.03 (vs. FT 3.01); F 0.12; T 0.19.
Partial/Adaptive Parameter Updates,LILAC (Lifelong Latent Actor-Critic),2024,Partial updates to latent variables in DP-MDPs for non-stationary dynamics.,L2Explorer (3D tasks).,A_N, FG, FT,SAC, L2D2-C,A_N ~0.80 (vs. SAC ~0.68); FG ~0.12; FT ~0.16.
Partial/Adaptive Parameter Updates,LiSP (Lifelong Skill Planning),2024,Partially updates skill policies with intrinsic rewards.,MineCraft/StarCraft II.,A_N, FG, SE,Standard planning, EWC,A_N ~0.77 (vs. EWC ~0.65); FG ~0.14; SE 30% better.
Partial/Adaptive Parameter Updates,Losse-FTL,2025,Sparse updates to dynamics models using FTL and locality-sensitive hashing.,High-dimensional control (robotic arms).,A_N, FG,HyperCRL, EWC,A_N ~0.82 (vs. HyperCRL ~0.75); FG ~0.11.
Partial/Adaptive Parameter Updates,CPPO (Continual PPO),2025,Selective updates with relabeling and weighting in PPO.,Lifelong Manipulation (10 tasks).,A_N, FG, SE,PPO, CLEAR,A_N ~0.79 (vs. PPO ~0.62); FG ~0.13; SE 25% better.
Partial/Adaptive Parameter Updates,L2D2-C,2025,Shared task-specific masks for selective updates in multi-agent settings.,Multi-agent environments (Hanabi variants).,A_N, FG, BT,HN-PPO, EWC,A_N ~0.81 (vs. HN-PPO ~0.70); FG ~0.10; BT ~0.12.
Partial/Adaptive Parameter Updates,DAAG (Decentralized Adaptive Agent Groups),2025,Adaptive grouping and replay for selective updates in multi-agent settings.,COOM (multi-agent sequences).,A_N, FG, FT,3RL, RePR,A_N ~0.84 (vs. 3RL ~0.76); FG ~0.09; FT ~0.19.
